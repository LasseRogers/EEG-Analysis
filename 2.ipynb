{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML and Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On grading\n",
    "\n",
    "This notebook has 5 different parts:\n",
    "\n",
    "  1. Visualization\n",
    "  2. Feature vector preparation + baseline\n",
    "  3. Train/test evaluation\n",
    "  4. Multiclass classification\n",
    "  5. KFold vs. Stratified KFold Cross-Validation\n",
    "  \n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "digits.data, digits.target, digits.images = shuffle(digits.data, digits.target,digits.images, random_state=1233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_image = digits.images[0]\n",
    "print(\"Digits is a N={} dataset of handwritten digits, stored as {} by {} gray-scale images\".format(\n",
    "digits.images.shape[0], first_image.shape[0], first_image.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like you can get a description of the dataset by executing the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: visualization\n",
    "\n",
    "#### Image representation\n",
    "\n",
    "An image is a 2-dimensional array of integers in the range 0-15. The value of a cell represents the intensity of the ink at that location, i.e. how black the cell should be rendered. A cell with a value of zero will be rendered as white, while a value of 15 signifies the darkest black. Values in between are grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess what digit this? Otherwise the representation below might help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(first_image, cmap='Greys', interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are supplied in the `target` attribute of the `digits` object. The labels are integers corresponding to the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Plot the first digit \"8\" in the dataset\n",
    "\n",
    "When you plot the image using the `.imshow` function, use the same parameters as we used before (`cmap='Greys', interpolation='none'`).\n",
    "\n",
    "##### Hint: It could be a good idea to use a function called `np.where` to find the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_image_eight = np.where(digits.target == 8)\n",
    "image_eight = digits.images[first_image_eight[0][0]]\n",
    "plt.imshow(image_eight, cmap='Greys', interpolation='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Average images for different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 10 digits in a 5x2 grid, top to bottom, left to right. I.e., start with `0` in the upper left corner, then `1` in the upper right corner, etc., until you get to `9` in the lower right corner.\n",
    "\n",
    "But watch out, you have to print the average image of a specific class, which means that you need to get all images of that class and then, average them (`np.mean`).\n",
    "\n",
    "Use the `fig` and `axes` data structure returned from the `plt.subplots` command. The `axes` is actually a two-dimensional array of subplots,  where subplots may be referenced by row and column indices. For instance, the plot in the upper right corner (at row 0 and column 1) can be accessed by\n",
    "\n",
    "````\n",
    "axes[0, 1]\n",
    "````\n",
    "\n",
    "The axis object supports all the usual plotting commands: `plot()`, `scatter()`, `hist()`, `imshow()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_image = [0]*10\n",
    "\n",
    "for digit in range(0,10):\n",
    "    index = np.where(digits.target == digit)\n",
    "    all_images = digits.images[index]\n",
    "    av_image[digit] = np.mean(all_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid outline of the numbers\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2)\n",
    "fig.set_size_inches(5, 15)\n",
    "\n",
    "# Plot a straight line in the upper right corner.\n",
    "# Replace by your own code\n",
    "#ax = axes[0, 1]\n",
    "#ax.plot([0, 1], [4, 5])\n",
    "#ax.set_title(\"Digit: 1\")\n",
    "\n",
    "r, k = 0,0\n",
    "for digit in range(0,10):\n",
    "    if digit%2 == 0:\n",
    "        k = 0\n",
    "        r = int(digit/2)\n",
    "    else:\n",
    "        k = 1\n",
    "        r = int((digit-1)/2)\n",
    "    ax = axes[r,k]\n",
    "    ax.imshow(av_image[digit], cmap='Greys', interpolation='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature vector preparation + baseline\n",
    "\n",
    "### Exercise 3: Flatten the images to 1-dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2-dimensional images must be converted to 1-dimensional feature vectors before they we can feed them to the machine learning algorithms in `sklearn`. The simplest (although perhaps not the best) way to do this is to string the rows together, such that the last element of the previous row would be adjacent to the first element of the next row. \n",
    "\n",
    "As an example, after this transformation the 3 by 2 array below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([[1,2], [3, 4], [5, 6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would become this 6 element array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([1,2, 3, 4, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Convert the `numbers_2d` array to a one-dimensional `numpy` array called `numbers_1d`. \n",
    "\n",
    "Hint: Use the `.reshape()` method on the array object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_2d = np.array([[1,2], [3, 4], [5, 6]])  \n",
    "\n",
    "# YOUR CODE HERE \n",
    "numbers_1d = numbers_2d.reshape(-1)\n",
    "print(numbers_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Reshape the entire images array, which has three dimensions, to a two-dimensional array. Save the converted array as `images_flat`. Check that \n",
    "\n",
    "```\n",
    "images_flat.shape\n",
    "```\n",
    "\n",
    "outputs \n",
    "\n",
    "```\n",
    "(1797, 64)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "images_flat = digits.images.reshape(1797, 64)\n",
    "\n",
    "images_flat.shape,digits.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare a baseline\n",
    "\n",
    "Before starting to perform experiments, it is a good practice to know how easy/hard it is to perform the specific task we want to perform. Also, once that we train our own classifiers (we will do it soon), we would like to know whether our classifiers are good enough.\n",
    "\n",
    "A common way is to establish a baseline, against which we will compare our new approach (*for instance a Perceptron or a Logistic Regression classifier trained on images*). Scikit-learn offers a set of very simple baselines that can be used to compare our own approaches against them, under the `sklearn.dummy.DummyClassifier` class. This classifier can be used in the same way as any other classifier in Scikit-learn.\n",
    "\n",
    "If the observed results between two classifiers (e.g. a baseline and a Perceptron) were similar, we would have to run significance tests to proof that one method is better than the other.",
    "\n",
    "### Exercise 4: Train a baseline and check how it works\n",
    "\n",
    "You should train a baseline that **makes predictions based on the training instances' class distribution**. There are several strategies that a Baseline can follow, check [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) to find the one that is relevant for this exercise. Just train a baseline with the whole dataset (`images_flat` and `targets`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "x = images_flat \n",
    "y = digits.target\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state = 42)\n",
    "dummy_clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already trained a dummy classifier (baseline). Please, make some predictions based on that classifier, and take a look at the outputs. Think about how it makes the predictions. If you want the predictions to be always the same, you have to establish the `random_state` parameter in the constructor (a seed number), together with the `strategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " > print (targets_pred)\n",
    "[1 5 3 ... 2 1 0]\n",
    " > print (targets)\n",
    "[0 8 7 ... 5 0 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally check other strategies and think about how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR PLAYFUL CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train/test evaluation\n",
    "\n",
    "### Exercise 5: Binary classification - digit `2` vs digit `1` (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment you will train a binary classifier to distinguish between the two digits `digit_a` and `digit_b`. Your classifier will thus only be trained on a subset of the images, namely those that have *either* class `digit_a` *or* class `digit_b`.\n",
    "\n",
    "We begin by choosing `1` and `2` as our two target labels (i.e., the digits we want to learn to distinguish from each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_a = 1\n",
    "digit_b = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now need to two do a couple of things to get your data ready for the classifier\n",
    "\n",
    "* Select the subset of flattened images and labels that have label `digit_a` or label `digit_b` and store them in new variables. These flattened images should be stored in a variable called `X`.\n",
    "* Change your labels, which will be a mix of the integers `digit_a` and `digit_b`, to -1 and 1. In this way, you will be able to also train the Perceptron that we saw in class. Although you can accomplish this in several ways, the most consistent approach is to use `LabelBinarizer`, which also works when you have more than two classes.\n",
    "* Split the data and labels into training and test sets using `train_test_split`. The test set should make up 25% of the instances. Supply the `random_state=42` parameter to the `train_test_split` command. This ensures that the split is the same every time you run the command, making it easier for us to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Your code here\n",
    "\n",
    "target_array_1 = np.where(digits.target==1)\n",
    "all_images_1 = []\n",
    "\n",
    "for row in target_array_1:\n",
    "    for cell in row:\n",
    "        all_images_1.append(images_flat[cell])\n",
    "    array_1=(np.array(all_images_1)).reshape(-1)\n",
    "    \n",
    "    \n",
    "target_array_2 = np.where(digits.target==2)\n",
    "all_images_2 = []\n",
    "\n",
    "for row in target_array_2:\n",
    "    for cell in row:\n",
    "        all_images_2.append(images_flat[cell])\n",
    "    array_2=(np.array(all_images_2)).reshape(-1)\n",
    "\n",
    "array1_2 = np.concatenate((array_1, array_2), axis=None)\n",
    "\n",
    "TC = LabelBinarizer()\n",
    "X = array1_2.reshape(359,64)\n",
    "y = np.array((np.count_nonzero(digits.target == digit_a, axis=0)*[1])+np.count_nonzero(digits.target == digit_b, axis=0)*[-1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "TC.fit(y)\n",
    "\n",
    "\n",
    "print (X.shape, y.shape, set(y))\n",
    "print (X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready then! How will the Perceptron work? Will Logistic Regression be better? You are the one that has to answer that question. Check how both of them work.\n",
    "\n",
    "Let's start by initializing the perceptron, by creating an object of a perceptron. Then, fit a model on the training set and obtain predictions on the test set. Then do the same with the Logistic regression. When you call the constructor of the Logistic Regression, please specify the `solver` parameter to `\"liblinear\"`.\n",
    "\n",
    "Assuming you named your predicted labels `y_pred_lr` and test labels `y_test`, you should be able to check the performance of your model by writing \n",
    "\n",
    "````\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "````\n",
    "\n",
    "You should also take a look at the confusion matrix.\n",
    "\n",
    "I am expecting the `classification report` and the `confusion matrix` of both the Logistic Regression and the Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron,LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "X = array1_2.reshape(359,64)\n",
    "y = np.array((np.count_nonzero(digits.target == digit_a, axis=0)*[1])+np.count_nonzero(digits.target == digit_b, axis=0)*[-1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "regression_1 = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "regression_1.fit(X,y)\n",
    "y_pred_lr = regression_1.predict(X_test) \n",
    "\n",
    "\n",
    "print (\"Logistic regression results\")\n",
    "print (classification_report(y_test,y_pred_lr))\n",
    "print (\"Confusion matrix: \")\n",
    "print (confusion_matrix(y_test,y_pred_lr))\n",
    "\n",
    "#YOUR CODE HERE\n",
    "p_1 = Perceptron()\n",
    "p_1.fit(X,y)\n",
    "y_pred_pcp = p_1.predict(X_test)\n",
    "\n",
    "print (\"Perceptron results\")\n",
    "print (classification_report(y_test,y_pred_pcp))\n",
    "print (\"Confusion matrix: \")\n",
    "print (confusion_matrix(y_test,y_pred_pcp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small note: Usually results are not so good :-(."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classifiers in `sklearn` come with multi-class support. For `LogisticRegression` or `Perceptron` (binary classifiers) multi-class is implemented as a series of binary classification decisions, where each decision is between some class $i$ and the remaining classes. E.g. is this digit a **7** or some other digit? This particular type of multi-class classification is referred to as *one-vs-all*.\n",
    "\n",
    "In this exercise, then, you are going to test both our own perceptron and our own averaged perceptron using that same strategy. To this end, you need to create one model for each class (is this a 1 or not, is this a 2 or not, and so on). Of course, you should split the dataset into a train/test set using the default parameters and setting the `random_state` value to `42` again. You should then report the average accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Multi-class classification in `sklearn` (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw how to perform multi-class classification. Luckily, in `sklearn`, all this takes place behind the scenes, and you are handed the final decision by the `predict()` method of the `Perceptron` object. Therefore, the `LabelEncoder` that we used in the previous exercise is not required, as `sklearn` is clever enough to manage this.\n",
    "\n",
    "In this exercise, you should train a `Perceptron` classifier directly on the complete set of flattened images and integer labels and print the classification report. Of course, you should split the dataset into a train/test set using the default parameters and setting the `random_state` value to `42` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "p = Perceptron(random_state=42)\n",
    "X = images_flat\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state =42)\n",
    "\n",
    "p.fit(X_train, y_train)\n",
    "y_pred = p.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print (\"X_train.shape,X_test.shape,y_train.shape,y_test.shape\")\n",
    "print (X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "print ()\n",
    "print (\"Classification report\")\n",
    "print (classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: KFold vs Stratified KFold\n",
    "\n",
    "### Exercise 7: Compare cross-validation strategies (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will be comparing two *cross-validation* strategies:\n",
    "\n",
    "- $k$-fold CV, which divides the dataset into $k$ parts. One by one, each of these parts are used as the test set, while the remaining $k-1$ parts constitute the training set. \n",
    "- Stratified $k$-fold CV, which works as above, but ensures that the proportion of instances of different classes in each part is the same (i.e., there is the same ratio of, say `1`s to `7`s).\n",
    "\n",
    "The two strategies are implemented as `KFold` and `StratifiedKFold` in `sklearn`. Both are imported in the cell below. \n",
    "\n",
    "We will consider $k=3$. For each of the two strategies, you should build a 3 by 10 pandas `DataFrame`, which, for each fold, shows the number of instances in each class. In this `DataFrame`, rows correspond to folds and columns to digit classes. \n",
    "\n",
    "When you initialize the `KFold` object, please set the `shuffle` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Your code for the KFold table here\n",
    "\n",
    "array = np.zeros(shape=(3,10))\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "dataframe = pd.DataFrame(array)\n",
    "dataframe.loc[1,1] = 1\n",
    "\n",
    "for count, [train, test] in enumerate(kf.split(digits.target),0):\n",
    "    test = digits.target[test]\n",
    "    for i in range(10):\n",
    "        for digit in test:\n",
    "            if digit == i:\n",
    "                dataframe.loc[count,i] += 1 \n",
    "                \n",
    "                \n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Your code for the StratifiedKFold table here\n",
    "\n",
    "array = np.zeros(shape=(3,10))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "dataframe = pd.DataFrame(array)\n",
    "dataframe.loc[1,1] = 1\n",
    "\n",
    "for count, [train, test] in enumerate(skf.split(images_flat, digits.target),0):\n",
    "    test = digits.target[test]\n",
    "    for i in range(10):\n",
    "        for digit in test:\n",
    "            if digit == i:\n",
    "                dataframe.loc[count,i] += 1\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the differences now?\n",
    "\n",
    "### Exercise 8: Evaluate models using KFold and Stratified KFold cross-validation\n",
    "\n",
    "Now, can you tell me how the Perceptron and the Logistic Regression work, evaluated using KFold cross-validation and Stratified KFold cross-validation. I would like to know the average accuracy for each classifier. Therefore, for each codeblock, you should return the average accuracy of each classifier (`Perceptron` and `LogisticRegression`.\n",
    "\n",
    "First, validate the models using KFold. `K` should be 3, `shuffle` should be `True` and `random_state` should be `42`.\n",
    "\n",
    "When you train the Logistic Regression, please set the `solver` parameter to `liblinear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron,LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "perceptron = Perceptron()\n",
    "acc_scores = []\n",
    "\n",
    "for train, test in kfold.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    y_prediction = perceptron.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_prediction)\n",
    "    acc_scores.append(accuracy)\n",
    "    \n",
    "logisticregression = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "acc_scores_lr = []\n",
    "\n",
    "for train, test in kfold.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    logisticregression.fit(X_train, y_train)\n",
    "    y_prediction = logisticregression.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_prediction)\n",
    "    acc_scores_lr.append(accuracy)\n",
    "\n",
    "avgpcp = np.mean(acc_scores)\n",
    "avglr = np.mean(acc_scores_lr)\n",
    "\n",
    "print (\"Average accuracy of Logistic Regression\", avglr)\n",
    "print (\"Average accuracy of Perceptron\", avgpcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "stratified_kfold  = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "perceptron = Perceptron()\n",
    "acc_scores = []\n",
    "\n",
    "for train, test in stratified_kfold.split(X,y):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    y_prediction = perceptron.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_prediction)\n",
    "    acc_scores.append(accuracy)\n",
    "    \n",
    "logisticregression = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "acc_scores_lr = []\n",
    "\n",
    "for train, test in stratified_kfold.split(X,y):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    logisticregression.fit(X_train, y_train)\n",
    "    y_prediction = logisticregression.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_prediction)\n",
    "    acc_scores_lr.append(accuracy)\n",
    "\n",
    "avgpcp = np.mean(acc_scores)\n",
    "avglr = np.mean(acc_scores_lr)\n",
    "\n",
    "print (\"Average accuracy of Logistic Regression\", avglr)\n",
    "print (\"Average accuracy of Perceptron\", avgpcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How well are we doing? Compare your results with state-of-the-art at Rodrigo Benenson's [object recognition benchmark page]( http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not challenged enough? (OPTIONAL)\n",
    "\n",
    "Does this model work with your own handwritten digits? Show me if it works. If it doesn't work so well, why does this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
